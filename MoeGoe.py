import gradio as gr
from scipy.io.wavfile import write
from mel_processing import spectrogram_torch
from text import text_to_sequence, _clean_text
from models import SynthesizerTrn
import utils
import commons
import torch
import re
import os
import tempfile
import logging
from torch import no_grad, LongTensor
import numpy as np

logging.getLogger('numba').setLevel(logging.WARNING)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œé…ç½®
model_global = None
hps_global = None
speakers_global = []
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# é»˜è®¤æ¨¡å‹è·¯å¾„
DEFAULT_MODEL_DIR = "/kaggle/working/ema"
DEFAULT_MODEL_PATH = os.path.join(DEFAULT_MODEL_DIR, "model.pth")  # è¯·æ ¹æ®å®é™…æ–‡ä»¶åä¿®æ”¹
DEFAULT_CONFIG_PATH = os.path.join(DEFAULT_MODEL_DIR, "config.json")  # è¯·æ ¹æ®å®é™…æ–‡ä»¶åä¿®æ”¹

def find_model_files(directory):
    """åœ¨ç›®å½•ä¸­æŸ¥æ‰¾æ¨¡å‹å’Œé…ç½®æ–‡ä»¶"""
    model_file = None
    config_file = None
    
    if os.path.exists(directory):
        for file in os.listdir(directory):
            if file.endswith('.pth'):
                model_file = os.path.join(directory, file)
            elif file.endswith('.json'):
                config_file = os.path.join(directory, file)
    
    return model_file, config_file

def load_model(model_path, config_path):
    """åŠ è½½VITSæ¨¡å‹å’Œé…ç½®"""
    global model_global, hps_global, speakers_global
    
    try:
        # å¦‚æœè·¯å¾„ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„
        if not model_path or not config_path:
            default_model, default_config = find_model_files(DEFAULT_MODEL_DIR)
            if default_model and default_config:
                model_path = default_model
                config_path = default_config
                status_msg = f"ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼š{os.path.basename(model_path)}\n"
            else:
                return None, "âŒ æœªæŒ‡å®šæ¨¡å‹æ–‡ä»¶ä¸”åœ¨é»˜è®¤è·¯å¾„æœªæ‰¾åˆ°æ¨¡å‹"
        else:
            status_msg = ""
        
        # åŠ è½½é…ç½®
        hps = utils.get_hparams_from_file(config_path)
        
        # è·å–è¯´è¯äººåˆ—è¡¨
        speakers = hps.speakers if 'speakers' in hps.keys() else ['0']
        n_speakers = hps.data.n_speakers if 'n_speakers' in hps.data.keys() else 0
        n_symbols = len(hps.symbols) if 'symbols' in hps.keys() else 0
        
        # åˆå§‹åŒ–æ¨¡å‹
        net_g = SynthesizerTrn(
            n_symbols,
            hps.data.filter_length // 2 + 1,
            hps.train.segment_size // hps.data.hop_length,
            n_speakers=n_speakers,
            **hps.model).to(device)
        
        net_g.eval()
        
        # ä¿®å¤ï¼šload_checkpointåªéœ€è¦ä¸¤ä¸ªå‚æ•°
        utils.load_checkpoint(model_path, net_g)
        
        # ä¿å­˜åˆ°å…¨å±€å˜é‡
        model_global = net_g
        hps_global = hps
        speakers_global = speakers
        
        return speakers, f"{status_msg}âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å‘ç° {len(speakers)} ä¸ªè¯´è¯äºº"
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}"

def get_speaker_list():
    """è·å–è¯´è¯äººåˆ—è¡¨ä¾›ä¸‹æ‹‰æ¡†ä½¿ç”¨"""
    if speakers_global:
        return [(f"{name} (ID:{idx})", idx) for idx, name in enumerate(speakers_global)]
    return [("æ— è¯´è¯äºº", 0)]

def process_text(text, length_scale, noise_scale, noise_scale_w):
    """å¤„ç†æ–‡æœ¬ä¸­çš„æ§åˆ¶æ ‡ç­¾"""
    if text is None or text == "":
        return None, length_scale, noise_scale, noise_scale_w, False
    
    # æå–æ§åˆ¶æ ‡ç­¾
    length_scale, text = get_label_value(text, 'LENGTH', length_scale, 'length scale')
    noise_scale, text = get_label_value(text, 'NOISE', noise_scale, 'noise scale')
    noise_scale_w, text = get_label_value(text, 'NOISEW', noise_scale_w, 'deviation of noise')
    cleaned, text = get_label(text, 'CLEANED')
    
    return text, length_scale, noise_scale, noise_scale_w, cleaned

def get_label_value(text, label, default, warning_name='value'):
    """ä»æ–‡æœ¬ä¸­æå–æ ‡ç­¾å€¼"""
    value = re.search(rf'\[{label}=(.+?)\]', text)
    if value:
        try:
            text = re.sub(rf'\[{label}=(.+?)\]', '', text, 1)
            value = float(value.group(1))
        except:
            print(f'Invalid {warning_name}!')
            value = default
    else:
        value = default
    return value, text

def get_label(text, label):
    """ä»æ–‡æœ¬ä¸­æå–å¸ƒå°”æ ‡ç­¾"""
    if f'[{label}]' in text:
        return True, text.replace(f'[{label}]', '')
    else:
        return False, text

def get_text(text, cleaned=False):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥"""
    if hps_global is None:
        return None
    
    if cleaned:
        text_norm = text_to_sequence(text, hps_global.symbols, [])
    else:
        text_norm = text_to_sequence(text, hps_global.symbols, hps_global.data.text_cleaners)
    
    if hps_global.data.add_blank:
        text_norm = commons.intersperse(text_norm, 0)
    
    text_norm = LongTensor(text_norm).to(device)
    return text_norm

def synthesize(text, speaker_id, length_scale, noise_scale, noise_scale_w, 
               model_path, config_path, output_path):
    """åˆæˆè¯­éŸ³çš„ä¸»å‡½æ•°"""
    
    # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½æ¨¡å‹
    if model_global is None or hps_global is None:
        speakers, load_result = load_model(model_path, config_path)
        if speakers is None:
            return None, load_result
    
    try:
        # å¤„ç†æ–‡æœ¬
        processed_text, length_scale, noise_scale, noise_scale_w, cleaned = process_text(
            text, length_scale, noise_scale, noise_scale_w
        )
        
        if processed_text is None or processed_text.strip() == "":
            return None, "è¯·è¾“å…¥æ–‡æœ¬"
        
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
        stn_tst = get_text(processed_text, cleaned=cleaned)
        if stn_tst is None:
            return None, "æ–‡æœ¬å¤„ç†å¤±è´¥"
        
        # æ¨ç†
        with no_grad():
            x_tst = stn_tst.unsqueeze(0)
            x_tst_lengths = LongTensor([stn_tst.size(0)]).to(device)
            sid = LongTensor([speaker_id]).to(device)
            
            audio = model_global.infer(
                x_tst, x_tst_lengths, sid=sid,
                noise_scale=noise_scale,
                noise_scale_w=noise_scale_w,
                length_scale=length_scale
            )[0][0, 0].data.cpu().float().numpy()
        
        # ä¿å­˜éŸ³é¢‘
        if not output_path:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_dir = tempfile.gettempdir()
            output_path = os.path.join(temp_dir, "vits_output.wav")
        
        write(output_path, hps_global.data.sampling_rate, audio)
        
        param_info = []
        if length_scale != 1.0:
            param_info.append(f"é•¿åº¦={length_scale:.2f}")
        if noise_scale != 0.667:
            param_info.append(f"å™ªå£°={noise_scale:.2f}")
        if noise_scale_w != 0.8:
            param_info.append(f"åå·®={noise_scale_w:.2f}")
        
        param_str = f"ï¼ˆ{', '.join(param_info)}ï¼‰" if param_info else ""
        
        return output_path, f"âœ… åˆæˆæˆåŠŸï¼{param_str}\néŸ³é¢‘å·²ä¿å­˜åˆ°ï¼š{output_path}"
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ åˆæˆå¤±è´¥ï¼š{str(e)}"

def update_speaker_dropdown(model_path, config_path):
    """æ›´æ–°è¯´è¯äººä¸‹æ‹‰æ¡†"""
    speakers, result = load_model(model_path, config_path)
    if speakers:
        speaker_list = [(f"{name} (ID:{idx})", idx) for idx, name in enumerate(speakers)]
        return gr.Dropdown(choices=speaker_list, value=0), result
    else:
        return gr.Dropdown(choices=[("æ— è¯´è¯äºº", 0)], value=0), result

def auto_load_default():
    """è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹"""
    if os.path.exists(DEFAULT_MODEL_DIR):
        model_file, config_file = find_model_files(DEFAULT_MODEL_DIR)
        if model_file and config_file:
            speakers, result = load_model(model_file, config_file)
            if speakers:
                speaker_list = [(f"{name} (ID:{idx})", idx) for idx, name in enumerate(speakers)]
                return (
                    model_file, config_file,
                    gr.Dropdown(choices=speaker_list, value=0),
                    f"âœ… å·²è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹ï¼š{os.path.basename(model_file)}"
                )
    return None, None, gr.Dropdown(choices=[("æ— è¯´è¯äºº", 0)], value=0), "æœªæ‰¾åˆ°é»˜è®¤æ¨¡å‹"

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="VITS TTS GUI", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ¤ VITS æ–‡æœ¬è½¬è¯­éŸ³ GUI
    
    åŸºäºVITSçš„æ–‡æœ¬åˆæˆè¯­éŸ³ç•Œé¢ï¼Œæ”¯æŒå¤šç§å‚æ•°è°ƒèŠ‚ã€‚
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            # æ¨¡å‹é…ç½®åŒº
            gr.Markdown("### ğŸ“ æ¨¡å‹é…ç½®")
            model_path = gr.File(
                label="é€‰æ‹©VITSæ¨¡å‹æ–‡ä»¶ (.pth)",
                file_types=[".pth"],
                type="filepath",
                value=DEFAULT_MODEL_PATH if os.path.exists(DEFAULT_MODEL_PATH) else None
            )
            config_path = gr.File(
                label="é€‰æ‹©é…ç½®æ–‡ä»¶ (.json)",
                file_types=[".json"],
                type="filepath",
                value=DEFAULT_CONFIG_PATH if os.path.exists(DEFAULT_CONFIG_PATH) else None
            )
            
            with gr.Row():
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                use_default_btn = gr.Button("ğŸ“‚ ä½¿ç”¨é»˜è®¤æ¨¡å‹", variant="secondary")
            
            load_status = gr.Textbox(label="åŠ è½½çŠ¶æ€", interactive=False, lines=3)
            
            gr.Markdown("### ğŸ›ï¸ åˆæˆå‚æ•°")
            length_scale = gr.Slider(
                minimum=0.1, maximum=2.0, value=1.0, step=0.1,
                label="é•¿åº¦ç¼©æ”¾ (LENGTH)",
                info="æ§åˆ¶è¯­é€Ÿï¼Œå€¼è¶Šå¤§è¯­é€Ÿè¶Šæ…¢"
            )
            noise_scale = gr.Slider(
                minimum=0.1, maximum=1.5, value=0.667, step=0.1,
                label="å™ªå£°ç¼©æ”¾ (NOISE)",
                info="æ§åˆ¶éšæœºæ€§ï¼Œå€¼è¶Šå¤§å˜åŒ–è¶Šå¤§"
            )
            noise_scale_w = gr.Slider(
                minimum=0.1, maximum=1.5, value=0.8, step=0.1,
                label="å™ªå£°åå·® (NOISEW)",
                info="æ§åˆ¶éŸ³è°ƒå˜åŒ–"
            )
        
        with gr.Column(scale=2):
            # è¾“å…¥è¾“å‡ºåŒº
            gr.Markdown("### ğŸ“ æ–‡æœ¬è¾“å…¥")
            text_input = gr.Textbox(
                label="è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬",
                placeholder="ä¾‹å¦‚ï¼š[LENGTH=1.2][NOISE=0.5]ä½ å¥½ï¼Œä¸–ç•Œï¼",
                lines=5,
                value="ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨VITSè¯­éŸ³åˆæˆï¼"
            )
            
            gr.Markdown("### ğŸ—£ï¸ è¯´è¯äººé€‰æ‹©")
            speaker_dropdown = gr.Dropdown(
                choices=[("è¯·å…ˆåŠ è½½æ¨¡å‹", 0)],
                value=0,
                label="é€‰æ‹©è¯´è¯äººID",
                info="ä»ä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©è¯´è¯äºº"
            )
            
            gr.Markdown("### ğŸ’¾ è¾“å‡ºè®¾ç½®")
            output_path = gr.Textbox(
                label="è¾“å‡ºéŸ³é¢‘è·¯å¾„",
                placeholder="ä¾‹å¦‚ï¼šoutput.wavï¼ˆç•™ç©ºåˆ™ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼‰",
                value=""
            )
            
            synthesize_btn = gr.Button("ğŸ”Š åˆæˆè¯­éŸ³", variant="primary", size="lg")
            
            with gr.Row():
                audio_output = gr.Audio(
                    label="åˆæˆç»“æœ",
                    type="filepath"
                )
            
            output_status = gr.Textbox(label="åˆæˆçŠ¶æ€", interactive=False, lines=3)

    # ç¤ºä¾‹åŒº
    gr.Markdown("### ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹")
    
    examples = gr.Examples(
        examples=[
            ["[ZH]ä½ å¥½ï¼Œä¸–ç•Œï¼[ZH]", 1.0, 0.667, 0.8],
            ["[LENGTH=1.5][ZH]è¿™æ˜¯ä¸€ä¸ªè¯­é€Ÿè¾ƒæ…¢çš„ç¤ºä¾‹[ZH]", 1.5, 0.667, 0.8],
            ["[NOISE=1.2][ZH]è¿™æ˜¯ä¸€ä¸ªéšæœºæ€§è¾ƒå¼ºçš„ç¤ºä¾‹[ZH]", 1.0, 1.2, 0.8],
            ["[LENGTH=0.8][NOISE=0.9][NOISEW=1.1][ZH]è¿™æ˜¯ä¸€ä¸ªç»„åˆå‚æ•°çš„ç¤ºä¾‹ZH]", 0.8, 0.9, 1.1],
        ],
        inputs=[text_input, length_scale, noise_scale, noise_scale_w],
        label="ç‚¹å‡»ç¤ºä¾‹å¿«é€Ÿå¡«å……"
    )
    
    gr.Markdown("""
    **è¾“å…¥æ ¼å¼è¯´æ˜ï¼š**
    - `[LENGTH=1.2]` - è®¾ç½®è¯­é€Ÿï¼ˆé»˜è®¤1.0ï¼‰
    - `[NOISE=0.5]` - è®¾ç½®å™ªå£°ï¼ˆé»˜è®¤0.667ï¼‰
    - `[NOISEW=0.9]` - è®¾ç½®å™ªå£°åå·®ï¼ˆé»˜è®¤0.8ï¼‰
    - `[CLEANED]` - ä½¿ç”¨å·²æ¸…æ´—æ–‡æœ¬
    
    **å®Œæ•´ç¤ºä¾‹ï¼š** `[LENGTH=1.2][NOISE=0.5][NOISEW=0.9][ZH]ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨VITSï¼[ZH]`
    **ä½¿ç”¨éœ€çŸ¥ï¼š** `è¾“å…¥æ—¶ä¸€å®šè¦è®°å¾—åœ¨éœ€è¦åˆæˆçš„æ–‡å­—ä¸¤è¾¹åŠ ä¸Šè¯­è¨€æ ‡è¯†ç¬¦ï¼Œä¾‹å¦‚[ZH]XXXXXX[ZH]`
    """)
    
    # äº‹ä»¶ç»‘å®š
    load_btn.click(
        fn=update_speaker_dropdown,
        inputs=[model_path, config_path],
        outputs=[speaker_dropdown, load_status]
    )
    
    use_default_btn.click(
        fn=auto_load_default,
        inputs=[],
        outputs=[model_path, config_path, speaker_dropdown, load_status]
    )
    
    synthesize_btn.click(
        fn=synthesize,
        inputs=[
            text_input, speaker_dropdown,
            length_scale, noise_scale, noise_scale_w,
            model_path, config_path, output_path
        ],
        outputs=[audio_output, output_status]
    )
    
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹
    demo.load(
        fn=auto_load_default,
        inputs=[],
        outputs=[model_path, config_path, speaker_dropdown, load_status]
    )

# å¯åŠ¨ç•Œé¢
if __name__ == "__main__":
    # æ£€æŸ¥é»˜è®¤æ¨¡å‹è·¯å¾„
    if os.path.exists(DEFAULT_MODEL_DIR):
        print(f"é»˜è®¤æ¨¡å‹ç›®å½•ï¼š{DEFAULT_MODEL_DIR}")
        model_file, config_file = find_model_files(DEFAULT_MODEL_DIR)
        if model_file and config_file:
            print(f"æ‰¾åˆ°é»˜è®¤æ¨¡å‹ï¼š{os.path.basename(model_file)}")
            print(f"æ‰¾åˆ°é»˜è®¤é…ç½®ï¼š{os.path.basename(config_file)}")
        else:
            print("é»˜è®¤ç›®å½•ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
    else:
        print(f"é»˜è®¤æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼š{DEFAULT_MODEL_DIR}")
    
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=True,
        show_error=True,
        allowed_paths=['/kaggle/working/ema']
    )
